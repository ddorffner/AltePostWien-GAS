{
  "65": {
    "inputs": {
      "guidance": 2.9000000000000004,
      "conditioning": [
        "67",
        0
      ]
    },
    "class_type": "FluxGuidance",
    "_meta": {
      "title": "VL FluxGuidance"
    }
  },
  "66": {
    "inputs": {
      "text": "dof, depth of field, blurry, overexposed, noise, noisy",
      "clip": [
        "252",
        0
      ]
    },
    "class_type": "CLIPTextEncode",
    "_meta": {
      "title": "VL Negative Prompt Gen"
    }
  },
  "67": {
    "inputs": {
      "text": [
        "69",
        0
      ],
      "clip": [
        "252",
        0
      ]
    },
    "class_type": "CLIPTextEncode",
    "_meta": {
      "title": "CLIP Text Encode (Positive Prompt)"
    }
  },
  "69": {
    "inputs": {
      "string_a": [
        "70",
        2
      ],
      "string_b": [
        "74",
        0
      ],
      "delimiter": ""
    },
    "class_type": "StringConcatenate",
    "_meta": {
      "title": "Concatenate"
    }
  },
  "70": {
    "inputs": {
      "text_input": "",
      "task": "more_detailed_caption",
      "fill_mask": true,
      "keep_model_loaded": false,
      "max_new_tokens": 400,
      "num_beams": 3,
      "do_sample": true,
      "output_mask_select": "",
      "seed": 1121003997678096,
      "image": [
        "156",
        0
      ],
      "florence2_model": [
        "71",
        0
      ]
    },
    "class_type": "Florence2Run",
    "_meta": {
      "title": "Florence2Run"
    }
  },
  "71": {
    "inputs": {
      "model": "gokaygokay/Florence-2-SD3-Captioner",
      "precision": "fp16",
      "attention": "eager",
      "convert_to_safetensors": false
    },
    "class_type": "DownloadAndLoadFlorence2Model",
    "_meta": {
      "title": "DownloadAndLoadFlorence2Model"
    }
  },
  "74": {
    "inputs": {
      "value": "perspective looking up, objects arranged neatly and repeatedly using available space"
    },
    "class_type": "PrimitiveString",
    "_meta": {
      "title": "VL Post Prompt Gen"
    }
  },
  "75": {
    "inputs": {
      "guidance": 3,
      "conditioning": [
        "66",
        0
      ]
    },
    "class_type": "FluxGuidance",
    "_meta": {
      "title": "VL FluxGuidance Negative"
    }
  },
  "77": {
    "inputs": {
      "size": 1800,
      "method": "BICUBIC",
      "image": [
        "235",
        0
      ]
    },
    "class_type": "ResizeLongestToNode",
    "_meta": {
      "title": "VL Depth Input Resize Longest"
    }
  },
  "101": {
    "inputs": {
      "image": [
        "263",
        0
      ]
    },
    "class_type": "GetImageSize+",
    "_meta": {
      "title": "ðŸ”§ Get Image Size"
    }
  },
  "106": {
    "inputs": {
      "clip_name": "sigclip_vision_patch14_384.safetensors"
    },
    "class_type": "CLIPVisionLoader",
    "_meta": {
      "title": "Load CLIP Vision"
    }
  },
  "108": {
    "inputs": {
      "downsampling_factor": 3,
      "downsampling_function": "bilinear",
      "mode": "keep aspect ratio",
      "weight": 0.5,
      "autocrop_margin": 0.1,
      "conditioning": [
        "127",
        0
      ],
      "style_model": [
        "115",
        0
      ],
      "clip_vision": [
        "106",
        0
      ],
      "image": [
        "156",
        0
      ]
    },
    "class_type": "ReduxAdvanced",
    "_meta": {
      "title": "VL Redux"
    }
  },
  "109": {
    "inputs": {
      "samples": [
        "113",
        0
      ],
      "vae": [
        "110",
        0
      ]
    },
    "class_type": "VAEDecode",
    "_meta": {
      "title": "VAE Decode"
    }
  },
  "110": {
    "inputs": {
      "vae_name": "flux-vae-bf16.safetensors"
    },
    "class_type": "VAELoader",
    "_meta": {
      "title": "Load VAE"
    }
  },
  "111": {
    "inputs": {
      "sampler_name": "euler"
    },
    "class_type": "KSamplerSelect",
    "_meta": {
      "title": "KSamplerSelect"
    }
  },
  "112": {
    "inputs": {
      "scheduler": "ddim_uniform",
      "steps": 55,
      "denoise": 1,
      "model": [
        "118",
        0
      ]
    },
    "class_type": "BasicScheduler",
    "_meta": {
      "title": "VL Scheduler"
    }
  },
  "113": {
    "inputs": {
      "noise": [
        "117",
        0
      ],
      "guider": [
        "124",
        0
      ],
      "sampler": [
        "111",
        0
      ],
      "sigmas": [
        "112",
        0
      ],
      "latent_image": [
        "116",
        0
      ]
    },
    "class_type": "SamplerCustomAdvanced",
    "_meta": {
      "title": "SamplerCustomAdvanced"
    }
  },
  "115": {
    "inputs": {
      "style_model_name": "flux1-redux-dev.safetensors"
    },
    "class_type": "StyleModelLoader",
    "_meta": {
      "title": "Load Style Model"
    }
  },
  "116": {
    "inputs": {
      "width": [
        "101",
        0
      ],
      "height": [
        "101",
        1
      ],
      "batch_size": 1
    },
    "class_type": "EmptySD3LatentImage",
    "_meta": {
      "title": "EmptySD3LatentImage"
    }
  },
  "117": {
    "inputs": {
      "noise_seed": 42123851236988
    },
    "class_type": "RandomNoise",
    "_meta": {
      "title": "VL Seed Gen"
    }
  },
  "118": {
    "inputs": {
      "max_shift": 1,
      "base_shift": 0.5,
      "width": [
        "101",
        0
      ],
      "height": [
        "101",
        1
      ],
      "model": [
        "165",
        0
      ]
    },
    "class_type": "ModelSamplingFlux",
    "_meta": {
      "title": "ModelSamplingFlux"
    }
  },
  "124": {
    "inputs": {
      "model": [
        "165",
        0
      ],
      "conditioning": [
        "143",
        0
      ]
    },
    "class_type": "BasicGuider",
    "_meta": {
      "title": "BasicGuider"
    }
  },
  "125": {
    "inputs": {
      "type": "depth",
      "control_net": [
        "128",
        0
      ]
    },
    "class_type": "SetShakkerLabsUnionControlNetType",
    "_meta": {
      "title": "Set Shakker Labs Union ControlNet Type"
    }
  },
  "127": {
    "inputs": {
      "strength": 0.4000000000000001,
      "start_percent": 0,
      "end_percent": 0.6,
      "positive": [
        "65",
        0
      ],
      "negative": [
        "75",
        0
      ],
      "control_net": [
        "125",
        0
      ],
      "image": [
        "263",
        0
      ],
      "vae": [
        "110",
        0
      ]
    },
    "class_type": "ControlNetApplyAdvanced",
    "_meta": {
      "title": "VL ControlNet Depth 1"
    }
  },
  "128": {
    "inputs": {
      "control_net_name": "controlnet-union_shakker.safetensors"
    },
    "class_type": "ControlNetLoader",
    "_meta": {
      "title": "Load ControlNet Model"
    }
  },
  "141": {
    "inputs": {
      "type": "depth",
      "control_net": [
        "142",
        0
      ]
    },
    "class_type": "SetShakkerLabsUnionControlNetType",
    "_meta": {
      "title": "Set Shakker Labs Union ControlNet Type"
    }
  },
  "142": {
    "inputs": {
      "control_net_name": "controlnet-union_shakker.safetensors"
    },
    "class_type": "ControlNetLoader",
    "_meta": {
      "title": "Load ControlNet Model"
    }
  },
  "143": {
    "inputs": {
      "strength": 0.5,
      "start_percent": 0,
      "end_percent": 0.7,
      "positive": [
        "108",
        0
      ],
      "negative": [
        "75",
        0
      ],
      "control_net": [
        "125",
        0
      ],
      "image": [
        "263",
        0
      ],
      "vae": [
        "110",
        0
      ]
    },
    "class_type": "ControlNetApplyAdvanced",
    "_meta": {
      "title": "VL ControlNet Depth 2"
    }
  },
  "156": {
    "inputs": {
      "size": 1024,
      "method": "NEAREST",
      "image": [
        "236",
        0
      ]
    },
    "class_type": "ResizeLongestToNode",
    "_meta": {
      "title": "VL Input Image Resize Longest"
    }
  },
  "164": {
    "inputs": {
      "ipadapter": "ip-adapter.bin",
      "clip_vision": "google/siglip-so400m-patch14-384",
      "provider": "cuda"
    },
    "class_type": "IPAdapterFluxLoader",
    "_meta": {
      "title": "Load IPAdapter Flux Model"
    }
  },
  "165": {
    "inputs": {
      "weight": 0.7,
      "start_percent": 0.1,
      "end_percent": 0.8,
      "model": [
        "251",
        0
      ],
      "ipadapter_flux": [
        "164",
        0
      ],
      "image": [
        "156",
        0
      ]
    },
    "class_type": "ApplyIPAdapterFlux",
    "_meta": {
      "title": "VL IPAdapter"
    }
  },
  "235": {
    "inputs": {
      "image": "C:\\ASSETS\\DepthMap\\DGN\\DGN.jpg"
    },
    "class_type": "LoadImageFromPath",
    "_meta": {
      "title": "VL Input Image Depth"
    }
  },
  "236": {
    "inputs": {
      "image": "C:\\Users\\SHA.ART\\Documents\\AltePost\\Footage\\IMG\\Quellsammlung\\2025-07-30 Quellbilder GEWÃ–LBE 2\\SELECTED_HOF\\Aldrin_Apollo_11.jpg"
    },
    "class_type": "LoadImageFromPath",
    "_meta": {
      "title": "VL Input Image Style"
    }
  },
  "246": {
    "inputs": {
      "value": "test-apollo-30_kaspect.png"
    },
    "class_type": "PrimitiveString",
    "_meta": {
      "title": "VL Output Path Original"
    }
  },
  "250": {
    "inputs": {
      "filepath": [
        "246",
        0
      ],
      "overwrite": true,
      "image": [
        "109",
        0
      ]
    },
    "class_type": "SaveImageDynamic",
    "_meta": {
      "title": "Save Image Dynamic"
    }
  },
  "251": {
    "inputs": {
      "unet_name": "flux1-dev-Q8_0.gguf"
    },
    "class_type": "UnetLoaderGGUF",
    "_meta": {
      "title": "VL Model Loader GGUF"
    }
  },
  "252": {
    "inputs": {
      "clip_name1": "clip_l.safetensors",
      "clip_name2": "t5xxl_fp8_e4m3fn.safetensors",
      "type": "flux",
      "device": "default"
    },
    "class_type": "DualCLIPLoader",
    "_meta": {
      "title": "DualCLIPLoader"
    }
  },
  "254": {
    "inputs": {
      "text_0": "The image is a close-up shot of a person dressed in an astronaut suit, standing on a rocky surface of the moon. The person is facing the right side of the image, with their left hand on their hip. The suit is white, with a black helmet on their head. The helmet has a large window on the front, and an American flag patch on the right arm of the helmet. There is a shadow of the person on the ground directly behind the person. The ground beneath the person is covered in a layer of snow, and there are patches of dirt and rocks scattered across the ground. The sky above the person's head is dark.perspective looking up, objects arranged neatly and repeatedly using available space",
      "text": [
        "69",
        0
      ]
    },
    "class_type": "ShowText|pysssss",
    "_meta": {
      "title": "VL Show Text"
    }
  },
  "255": {
    "inputs": {
      "images": [
        "236",
        0
      ]
    },
    "class_type": "PreviewImage",
    "_meta": {
      "title": "Preview Image"
    }
  },
  "256": {
    "inputs": {
      "value": "prompt456"
    },
    "class_type": "PrimitiveString",
    "_meta": {
      "title": "VL Post Prompt Gen"
    }
  },
  "263": {
    "inputs": {
      "blur_radius": 1,
      "sigma": 1,
      "image": [
        "77",
        0
      ]
    },
    "class_type": "ImageBlur",
    "_meta": {
      "title": "VL Image Blur Depth"
    }
  },
  "264": {
    "inputs": {
      "images": [
        "263",
        0
      ]
    },
    "class_type": "PreviewImage",
    "_meta": {
      "title": "Preview Image"
    }
  },
  "265": {
    "inputs": {
      "images": [
        "109",
        0
      ]
    },
    "class_type": "PreviewImage",
    "_meta": {
      "title": "Preview Image"
    }
  }
}